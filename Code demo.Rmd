---
title: "Code Demo"
author: "Yupeng Miao"
date: "2020/10/14"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library(ggplot2)
library(caret)
library(xlsx)
```

# Load the dataset (No NA entry)
```{r}
cad <- read.xlsx("DataWrangling/Z-Alizadeh sani dataset.xlsx", 1, header=TRUE)
head(cad)
```
# 1 Data Wrangling
## 1.1 Factorising variables in character
```{r}
for (i in 1:length(cad)) {
  if (class(cad[,i]) == "character"){
    cad[,i] <- as.factor(cad[,i])
  }
  else if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
str(cad)
```
### We observe that the feature Exertional CP only has one level, so we drop it from the dataset
```{r}
cad <- subset(cad, select = -Exertional.CP)
```
## set up the dataset for svm
```{r}
svm.df <- cad
```

## 1.2 Catrgorical Random Variables: One-Hot Encoding
```{r}
dummy <- dummyVars(" ~ .", data = cad)
cad <- data.frame(predict(dummy, newdata = cad))
head(cad)
```

## 1.3 Remove Zero Variance or Near-zero variance variable
```{r}
nz <- nearZeroVar(cad, saveMetrics = TRUE)
nz[nz$nzv,]
nz.svm <- nearZeroVar(svm.df, saveMetrics = T)
nz.svm[nz.svm$nzv,]
```

### There are 32 near-zero-variance features which may cause problems when the data are split into cross validation or boostraping samples, we eliminated those features.
```{r}
nzv <- nearZeroVar(cad)
cad <- cad[,-nzv]
nzv.svm <- nearZeroVar(svm.df)
svm.df <- svm.df[,-nzv.svm]
str(cad)
str(svm.df)
```

## 1.4 Remove Correlated predictors
```{r}
cad_cor <- cor(cad)
high.cor <- sum(abs(cad_cor[upper.tri(cad_cor)])>0.999)
summary(cad_cor[upper.tri(cad_cor)])
```

### We observed 17 highly correlated features, most of them are generated by one-hot encoding (e.g. for a variable with 3 levels, drop one of the dummies will not lose any information), only one of them (neut) isn't a column generated by One-Hot Encoding. we drop them and the effect of removing those with absolute correlations above 0.75 are shown below:
```{r}
high.cor.feature <- findCorrelation(cad_cor, cutoff = 0.75)
high.cor.feature[1] = 65 #remove the Cath.Normal rather than Cath.cad
cad <- cad[, -high.cor.feature]
cad_cor2 <- cor(cad)
summary(cad_cor2[upper.tri(cad_cor2)])
```
```{r}
svm.df <- svm.df[, -c(36, 65)]
```

## 1.5 Features with linear dependencies:
```{r}
ld <- findLinearCombos(cad)
ld
```
###Nothing observed so we are ok.

## 1.6 factorising binary variables:
```{r}
for (i in 1:length(cad)) {
  if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
str(cad)
```

# 2 Train/Test split:

```{r}
set.seed(3164)
train.index <- createDataPartition(cad$Cath.Cad, times = 1, p = 0.8, list = FALSE)
cad.train <- cad[train.index,]
cad.test <- cad[-train.index,]
```

# 3 Set up 10-cross-validation
```{r}
parameters = trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 10,
                          classProbs = FALSE)
```

# 4 Feature selection by SVM
```{r}
# train SVM
SVM = train(Cath.Cad ~ ., data = cad.train,
            method = "svmPoly",
            trControl = parameters,
            tuneGrid = data.frame(degree = 1, scale = 1, C = 1),
            preProcess = c("pca", "scale", "center"),
            na.action = na.omit)

# Ranking features by Importance
feature.rank = varImp(SVM, scale = TRUE)
plot(feature.rank)
```

### Generate a dataset of features with importance > 40
```{r}
important <- c(feature.rank$importance$X0>40, TRUE)
im <- cad[,important]
str(im)
# train/test split
im.train <- im[train.index,]
im.test <- im[-train.index,]
```
```{r}
svm.df <- subset(svm.df, select = c(Age, DM, HTN, BP, Typical.Chest.Pain, Atypical, FBS, TG, ESR, K, EF.TTE, Region.RWMA, Cath))
```

# 3 Claasifier Training and Testing
## 3.1 Initialising training settings
```{r}
# set up results table
result = data.frame(Classifier = c("SVM.Poly", "SVM.Radial", "SVM.Linear", "Random Forest", "Neurual Network", "Naive Bayes", "Logistic Regression", "LDA", "KNN", "GBM", "Decision Tree","AdaBoost Classification Tree", "Boosted Logistic Regression"), Training_ROC = 0:12, Testing_Accuracy = 0:12, Test_AUC = 0:12)
```
```{r}
control = trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 10,
                       classProbs = TRUE,
                       summaryFunction = twoClassSummary
                       )
```
```{r}
# Test/ train split for SVM
set.seed(3164)
train.index.svm <- createDataPartition(svm.df$Cath, times = 1, p = 0.8, list = FALSE)
svm.train <- svm.df[train.index.svm,]
svm.test <- svm.df[-train.index.svm,]
```

# 3.2 SVM
## 3.2.1 SVM Polynomial
```{r}
set.seed(3164)
# Train SVM polynomial with ROC metric
svm.poly= train(Cath ~ ., data = svm.train,
            method = "svmPoly",
            trControl = control,
            tuneGrid = data.frame(degree = c(1,1), scale = c(1,2), C = c(1,3)),
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.poly$results)
```
```{r}
# Train without ROC
SVM = train(Cath.Cad ~ ., data = im.train,
            method = "svmPoly",
            trControl = parameters,
            tuneGrid = data.frame(degree = c(1,1), scale = c(1,2), C = c(1,3)),
            preProcess = c("pca", "scale", "center"),
            na.action = na.omit)
print(SVM$results)
```

```{r}
# Test SVM polynomial with ROC
svm.poly.predict = predict(svm.poly, svm.test)
# confusion matrix
cm.svm.poly <- confusionMatrix(svm.poly.predict, svm.test$Cath)
print(cm.svm.poly)
```
```{r}
# Test SVM polynomial without ROC
svm.predict = predict(SVM, im.test)
# confusion matrix
cm.svm <- confusionMatrix(svm.predict, im.test$Cath.Cad)
print(cm.svm)
```
### We notice that training classifier with ROC as metric yields higher testing accuracy, thus, we'll train the left classifiers with ROC.
```{r}
result$ROC[1] = max(svm.poly$results$ROC)
result$Testing_Accuracy[1] = cm.svm.poly$overall[1]
```

## 3.2.2 SVM Radial
```{r}
set.seed(3164)
# Train SVM Radial
svm.rad= train(Cath ~ ., data = svm.train,
            method = "svmRadial",
            trControl = control,
            tuneLength = 10,
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.rad$results)
```
```{r}
# Test SVM Radial
svm.rad.predict = predict(svm.rad, svm.test)
# confusion matrix
cm.svm.rad <- confusionMatrix(svm.rad.predict, svm.test$Cath)
print(cm.svm.rad)
```
```{r}
result$ROC[2] = max(svm.rad$results$ROC)
result$Testing_Accuracy[2] = cm.svm.rad$overall[1]
```

## 3.2.3 SVM linear
```{r}
set.seed(3164)
# Train SVM linear
svm.l= train(Cath ~ ., data = svm.train,
            method = "svmLinear",
            trControl = control,
            tuneLength = 10,
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.l$results)
```
```{r}
# Test SVM linear
svm.l.predict = predict(svm.l, svm.test)
# confusion matrix
cm.svm.l <- confusionMatrix(svm.l.predict, svm.test$Cath)
print(cm.svm.l)
```
```{r}
result$ROC[3] = max(svm.l$results$ROC)
result$Testing_Accuracy[3] = cm.svm.l$overall[1]
```

```{r}
library("pROC")
```

```{r}
svm.poly.probs = predict(svm.poly,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
svm.rad.probs = predict(svm.rad,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
svm.l.probs = predict(svm.l,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
# plot ROC
svm.poly.ROC = roc(response = svm.test$Cath,
              predictor = svm.poly.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)

svm.rad.ROC = roc(response = svm.test$Cath,
              predictor = svm.rad.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)

svm.l.ROC = roc(response = svm.test$Cath,
              predictor = svm.l.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
```
```{r}
result$Test_AUC[1] = svm.poly.ROC$auc
result$Test_AUC[2] = svm.rad.ROC$auc
result$Test_AUC[3] = svm.l.ROC$auc
```

#3.4 Random Forest
```{r}
# train the random forest
rf <- train(Cath ~., data = svm.train,
                 method = "rf",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(rf)
```
```{r}
# Test the random forest
rf.predict = predict(rf, svm.test)
# confusion matrix
cm.rf <- confusionMatrix(rf.predict, svm.test$Cath)
print(cm.rf)
```

```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[4] = max(rf$results$ROC)
result$Testing_Accuracy[4] = cm.rf$overall[1]

# Save the testing ROC for plotting later
rf.probs = predict(rf,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
rf.ROC = roc(response = svm.test$Cath,
              predictor = rf.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[4] = rf.ROC$auc
```
3.5 Neural Network
```{r}
# train the neural network
cath_index = grep("Cath", colnames(svm.train))
nn = train(svm.train[, -cath_index], 
            svm.train$Cath,
            method = "nnet",
            trControl = control,
            preProcess = c("scale", "center"),
            tuneLength = 5,
            verbose = F,
            trace = F,
            metric = "ROC",
            na.action = na.omit)
print(nn)
```
```{r}
# testing the neural network
nn.predict = predict(nn, svm.test)
# confusion matrix
cm.nn <- confusionMatrix(nn.predict, svm.test$Cath)
print(cm.nn)
```

```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[5] = max(nn$results$ROC)
result$Testing_Accuracy[5] = cm.nn$overall[1]
# plot the testing ROC
nn.probs = predict(nn,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
nn.ROC = roc(response = svm.test$Cath,
              predictor = nn.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[5] = nn.ROC$auc
```
3.6 Naive Bayes
```{r message = FALSE}
set.seed(3164)
# train Naive Bayes
nb <- train(Cath ~., data = svm.train,
                 method = "nb",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneGrid = data.frame(fL = 0, usekernel = T, adjust = 1),
                 metric = "ROC")
print(nb)
```
```{r message=FALSE}
# Test the Naive Bayes
nb.predict = predict(nb, svm.test)
# confusion matrix
cm.nb <- confusionMatrix(nb.predict, svm.test$Cath)
print(cm.nb)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[6] = max(nb$results$ROC)
result$Testing_Accuracy[6] = cm.nb$overall[1]

# Save the testing ROC for plotting later
nb.probs = predict(nb,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
nb.ROC = roc(response = svm.test$Cath,
              predictor = nb.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[6] = nb.ROC$auc
```

# 3.7 Logistic Regression
```{r}
set.seed(3164)
# train the logistic regression
lr <- train(Cath ~., data = svm.train,
                 method = "glm",
                 family = "binomial",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(lr)
```
```{r}
# Test the logistic regression
lr.predict = predict(lr, svm.test)
# confusion matrix
cm.lr <- confusionMatrix(lr.predict, svm.test$Cath)
print(cm.lr)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[7] = max(lr$results$ROC)
result$Testing_Accuracy[7] = cm.lr$overall[1]

# Save the testing ROC for plotting later
lr.probs = predict(lr,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
lr.ROC = roc(response = svm.test$Cath,
              predictor = lr.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[7] = lr.ROC$auc
```

# 3.8 LDA
```{r}
set.seed(3164)
# train the LDA
lda <- train(Cath ~., data = svm.train,
                 method = "lda",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(lda)
```
```{r}
# Test the lda
lda.predict = predict(lda, svm.test)
# confusion matrix
cm.lda <- confusionMatrix(lda.predict, svm.test$Cath)
print(cm.lda)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[8] = max(lda$results$ROC)
result$Testing_Accuracy[8] = cm.lda$overall[1]

# Save the testing ROC for plotting later
lda.probs = predict(lda,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
lda.ROC = roc(response = svm.test$Cath,
              predictor = lda.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[8] = lda.ROC$auc
```
3.8 KNN
```{r}
set.seed(3164)
# train the knn
knn <- train(Cath ~., data = svm.train,
                 method = "knn",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(knn)
```
```{r}
# Test the knn
knn.predict = predict(knn, svm.test)
# confusion matrix
cm.knn <- confusionMatrix(knn.predict, svm.test$Cath)
print(cm.knn)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[9] = max(knn$results$ROC)
result$Testing_Accuracy[9] = cm.knn$overall[1]

# Save the testing ROC for plotting later
knn.probs = predict(knn,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
knn.ROC = roc(response = svm.test$Cath,
              predictor = knn.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[9] = knn.ROC$auc
```
3.10 GBM
```{r}
set.seed(3164)
# train the GBM
gbm <- train(Cath ~., data = svm.train,
                 method = "gbm",
                 trControl = control,
                 verbose = FALSE,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(gbm)
```
```{r}
# Test the gbm
gbm.predict = predict(gbm, svm.test)
# confusion matrix
cm.gbm <- confusionMatrix(gbm.predict, svm.test$Cath)
print(cm.gbm)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[10] = max(gbm$results$ROC)
result$Testing_Accuracy[10] = cm.gbm$overall[1]

# Save the testing ROC for plotting later
gbm.probs = predict(gbm,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
gbm.ROC = roc(response = svm.test$Cath,
              predictor = gbm.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[10] = gbm.ROC$auc
```
# Decision Tree
```{r message=FALSE}
set.seed(3164)
# train the decision tree
dt <- train(Cath ~., data = svm.train,
                 method = "C5.0",
                 trControl = control,
                 verbose = FALSE,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(dt)
```
```{r}
# Test the decision tree
dt.predict = predict(dt, svm.test)
# confusion matrix
cm.dt <- confusionMatrix(dt.predict, svm.test$Cath)
print(cm.dt)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[11] = max(dt$results$ROC)
result$Testing_Accuracy[11] = cm.dt$overall[1]

# Save the testing ROC for plotting later
dt.probs = predict(dt,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
dt.ROC = roc(response = svm.test$Cath,
              predictor = dt.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[11] = dt.ROC$auc
```
# 3.12 AdaBoost Classification tree
```{r}
set.seed(3164)
# train adaBoost Classification Tree
adaB = train(Cath ~ ., data = svm.train,
            method = "AdaBoost.M1",
            tuneGrid = data.frame(mfinal = (1:3)*5, maxdepth = c(5,5,5), coeflearn = c("Breiman", "Freund", "Zhu")),
            preProcess = c("scale", "center"),
            na.action = na.omit)
print(adaB)
```


```{r}
# Testing AdaBoost Classification tree
adab.tree.predict = predict(adaB, svm.test)
# confusion matrix
cm.adab.tree <- confusionMatrix(adab.tree.predict, svm.test$Cath)
print(cm.adab.tree)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[12] = paste(as.character(max(adaB$results$Accuracy)), "(Accuracy)", sep = " ")
result$Testing_Accuracy[12] = cm.adab.tree$overall[1]

# Save the testing ROC for plotting later
adab.probs = predict(adaB,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
adab.ROC = roc(response = svm.test$Cath,
              predictor = adab.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[12] = adab.ROC$auc
```
# 3.12 Boosted Logistic Regression
```{r}
set.seed(3164)
# train Boosted Logistic Regression
blr = train(Cath ~ ., data = svm.train,
            method = "LogitBoost",
            tuneGrid = data.frame(nIter = c(5,10,20,50)),
            trControl = control,
            preProcess = c("scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(blr)
```

```{r}
# Testing Boosted logistic regression
blr.predict = predict(blr, svm.test)
# confusion matrix
cm.blr <- confusionMatrix(blr.predict, svm.test$Cath)
print(cm.blr)
```
```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[13] = max(blr$results$ROC)
result$Testing_Accuracy[13] = cm.blr$overall[1]

# Save the testing ROC for plotting later
blr.probs = predict(blr,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
blr.ROC = roc(response = svm.test$Cath,
              predictor = blr.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
result$Test_AUC[13] = blr.ROC$auc
```
4 Review the Results and Plotting the ROC
```{r}
# view the results
print(result)
```
```{r}
# ploting the ROC
plot(svm.poly.ROC,type = "S",col = "#C0392B")
plot(svm.rad.ROC,add = TRUE,col = "#E74C3C")
plot(svm.l.ROC,add = TRUE,col = "#D35400")
plot(rf.ROC,add = TRUE,col = "#9B59B6")
plot(nn.ROC,add = TRUE,col = "#8E44AD")
plot(nb.ROC,add = TRUE,col = "#2980B9")
plot(lr.ROC,add = TRUE,col = "#3498DB")
plot(lda.ROC,add = TRUE,col = "#1ABC9C")
plot(knn.ROC,add = TRUE,col = "#16A085")
plot(gbm.ROC,add = TRUE,col = "#27AE60")
plot(dt.ROC,add = TRUE,col = "#2ECC71")
plot(adab.ROC,add = TRUE,col = "#F1C40F")
plot(blr.ROC,add = TRUE,col = "#F39C12")

legend("bottomright", legend = c("svm.poly","svm.rad", "svm.l", "Random Forest", "Neural Network", "Naive Bayes", 
                                 "Logistic Regression", "LDA", "KNN", "GBM", "Decision Tree", "AdaBoost CT", "Boosted Logistic regression"), 
       col = c("#C0392B", "#E74C3C", "#D35400", "#9B59B6", "#8E44AD", "#2980B9", "#3498DB", "#1ABC9C", "#16A085", "#27AE60", "#2ECC71", 
               "#F1C40F", "#F39C12"),lwd = 2, cex = 0.8)
```
# 5 Combining the models
```{r}
# generate a dataframe that contains the prediction results of all models
generate_ensemble_df <- function(caddataset){
  #Input: caddataset - the training dataset
  #Output: a data frame of size (nrow x 12). Each feature (column) is comprised of the predictions made by that specific model, in probabilities.
  #Runtime: Linear
  
  aggregate_pred.df <- as.data.frame(caddataset$Cath)
  colnames(aggregate_pred.df)=c("Cath")
  #attach predictions
  aggregate_pred.df$knnres <-  predict(knn, caddataset, type = "prob")$Cad
  aggregate_pred.df$ldares <-  predict(lda, caddataset, type = "prob")$Cad
  aggregate_pred.df$lrres <-  predict(lr, caddataset, type = "prob")$Cad
  aggregate_pred.df$rfres <-  predict(rf, caddataset, type = "prob")$Cad
  aggregate_pred.df$svmLres <-  predict(svm.l, caddataset, type = "prob")$Cad
  aggregate_pred.df$svmPres <-  predict(svm.poly, caddataset, type = "prob")$Cad
  aggregate_pred.df$svmRres <-  predict(svm.rad, caddataset, type = "prob")$Cad
  aggregate_pred.df$NNres <-  predict(nn, caddataset, type = "prob")$Cad
  aggregate_pred.df$GBMres <-  predict(gbm, caddataset, type = "prob")$Cad
  aggregate_pred.df$AdaBres <-  predict(adaB, caddataset, type = "prob")$Cad
  aggregate_pred.df$dtres <-  predict(dt, caddataset, type = "prob")$Cad
  aggregate_pred.df$blrres <-  predict(blr, caddataset, type = "prob")$Cad
  return(aggregate_pred.df)
}
```
## 5.1 Naive Voting Ensemble
```{r}
vote_ensemble <- function(dataset, label="Cath"){
  #Input: dataset - Any data set. 
  #Input label - A column name to predict values for.
  #Output: a vector containing the average value of all features for each input row.
  #Converts Y or N in input df to 1 and 0 respectively.
  #To be used to take a unweighted vote of columns, aka. vote ensembling when combined with generate_ensemble_df
  
  df = dataset[,names(dataset) != c(label)]
  num = dim(df)[2]
  vote = apply(df, 1, function(x) sum(as.numeric(x)))/num
  return(as.factor(ifelse(round(vote) == 0,"N","Y")))
}
```














