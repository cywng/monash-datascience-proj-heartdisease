---
title: "Code Demo"
author: "Yupeng Miao"
date: "2020/10/14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(caret)
library(xlsx)
```

# Load the dataset (No NA entry)
```{r}
cad <- read.xlsx("DataWrangling/Z-Alizadeh sani dataset.xlsx", 1, header=TRUE)
head(cad)
```
# 1 Data Wrangling
## 1.1 Factorising variables in character
```{r}
for (i in 1:length(cad)) {
  if (class(cad[,i]) == "character"){
    cad[,i] <- as.factor(cad[,i])
  }
  else if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
str(cad)
```
### We observe that the feature Exertional CP only has one level, so we drop it from the dataset
```{r}
cad <- subset(cad, select = -Exertional.CP)
```
## set up the dataset for svm
```{r}
svm.df <- cad
```

## 1.2 Catrgorical Random Variables: One-Hot Encoding
```{r}
dummy <- dummyVars(" ~ .", data = cad)
cad <- data.frame(predict(dummy, newdata = cad))
head(cad)
```

## 1.3 Remove Zero Variance or Near-zero variance variable
```{r}
nz <- nearZeroVar(cad, saveMetrics = TRUE)
nz[nz$nzv,]
nz.svm <- nearZeroVar(svm.df, saveMetrics = T)
nz.svm[nz.svm$nzv,]
```

### There are 32 near-zero-variance features which may cause problems when the data are split into cross validation or boostraping samples, we eliminated those features.
```{r}
nzv <- nearZeroVar(cad)
cad <- cad[,-nzv]
nzv.svm <- nearZeroVar(svm.df)
svm.df <- svm.df[,-nzv.svm]
str(cad)
str(svm.df)
```

## 1.4 Remove Correlated predictors
```{r}
cad_cor <- cor(cad)
high.cor <- sum(abs(cad_cor[upper.tri(cad_cor)])>0.999)
summary(cad_cor[upper.tri(cad_cor)])
```

### We observed 17 highly correlated features, most of them are generated by one-hot encoding (e.g. for a variable with 3 levels, drop one of the dummies will not lose any information), only one of them (neut) isn't a column generated by One-Hot Encoding. we drop them and the effect of removing those with absolute correlations above 0.75 are shown below:
```{r}
high.cor.feature <- findCorrelation(cad_cor, cutoff = 0.75)
high.cor.feature[1] = 65 #remove the Cath.Normal rather than Cath.cad
cad <- cad[, -high.cor.feature]
cad_cor2 <- cor(cad)
summary(cad_cor2[upper.tri(cad_cor2)])
```
```{r}
svm.df <- svm.df[, -c(36, 65)]
```

## 1.5 Features with linear dependencies:
```{r}
ld <- findLinearCombos(cad)
ld
```
###Nothing observed so we are ok.

## 1.6 factorising binary variables:
```{r}
for (i in 1:length(cad)) {
  if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
str(cad)
```

# 2 Train/Test split:

```{r}
set.seed(3164)
train.index <- createDataPartition(cad$Cath.Cad, times = 1, p = 0.8, list = FALSE)
cad.train <- cad[train.index,]
cad.test <- cad[-train.index,]
```

# 3 Set up 10-cross-validation
```{r}
parameters = trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 10,
                          classProbs = FALSE)
```

# 4 Feature selection by SVM
```{r}
# train SVM
SVM = train(Cath.Cad ~ ., data = cad.train,
            method = "svmPoly",
            trControl = parameters,
            tuneGrid = data.frame(degree = 1, scale = 1, C = 1),
            preProcess = c("pca", "scale", "center"),
            na.action = na.omit)

# Ranking features by Importance
feature.rank = varImp(SVM, scale = TRUE)
plot(feature.rank)
```

### Generate a dataset of features with importance > 40
```{r}
important <- c(feature.rank$importance$X0>40, TRUE)
im <- cad[,important]
str(im)
# train/test split
im.train <- im[train.index,]
im.test <- im[-train.index,]
```
```{r}
svm.df <- subset(svm.df, select = c(Age, DM, HTN, BP, Typical.Chest.Pain, Atypical, FBS, TG, ESR, K, EF.TTE, Region.RWMA, Cath))
```

# 3 Claasifier Training and Testing
## 3.1 Initialising training settings
```{r}
# set up results table
result = data.frame(Classifier = c("SVM.Poly", "SVM.Radial", "SVM.Linear", "Random Forest", "Neurual Network", "Naive Bayes", "Logistic Regression", "LDA", "KNN", "GBM", "Decision Tree","AdaBoost Classification Tree", "Boosted Logistic Regression"), ROC = 0:12, Testing_Accuracy = 0:12, Test_AUC = 0:12)
```
```{r}
control = trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 10,
                       classProbs = TRUE,
                       summaryFunction = twoClassSummary
                       )
```
```{r}
# Test/ train split for SVM
set.seed(3164)
train.index.svm <- createDataPartition(svm.df$Cath, times = 1, p = 0.8, list = FALSE)
svm.train <- svm.df[train.index.svm,]
svm.test <- svm.df[-train.index.svm,]
```

# 3.2 SVM
## 3.2.1 SVM Polynomial
```{r}
set.seed(3164)
# Train SVM polynomial with ROC metric
svm.poly= train(Cath ~ ., data = svm.train,
            method = "svmPoly",
            trControl = control,
            tuneGrid = data.frame(degree = c(1,1), scale = c(1,2), C = c(1,3)),
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.poly$results)
```
```{r}
# Train without ROC
SVM = train(Cath.Cad ~ ., data = im.train,
            method = "svmPoly",
            trControl = parameters,
            tuneGrid = data.frame(degree = c(1,1), scale = c(1,2), C = c(1,3)),
            preProcess = c("pca", "scale", "center"),
            na.action = na.omit)
print(SVM$results)
```

```{r}
# Test SVM polynomial with ROC
svm.poly.predict = predict(svm.poly, svm.test)
# confusion matrix
cm.svm.poly <- confusionMatrix(svm.poly.predict, svm.test$Cath)
print(cm.svm.poly)
```
```{r}
# Test SVM polynomial without ROC
svm.predict = predict(SVM, im.test)
# confusion matrix
cm.svm <- confusionMatrix(svm.predict, im.test$Cath.Cad)
print(cm.svm)
```
### We notice that training classifier with ROC as metric yields higher testing accuracy, thus, we'll train the left classifiers with ROC.
```{r}
result$ROC[1] = max(svm.poly$results$ROC)
result$Testing_Accuracy[1] = cm.svm.poly$overall[1]
```

## 3.2.2 SVM Radial
```{r}
set.seed(3164)
# Train SVM Radial
svm.rad= train(Cath ~ ., data = svm.train,
            method = "svmRadial",
            trControl = control,
            tuneLength = 10,
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.rad$results)
```
```{r}
# Test SVM Radial
svm.rad.predict = predict(svm.rad, svm.test)
# confusion matrix
cm.svm.rad <- confusionMatrix(svm.rad.predict, svm.test$Cath)
print(cm.svm.rad)
```
```{r}
result$ROC[2] = max(svm.rad$results$ROC)
result$Testing_Accuracy[2] = cm.svm.rad$overall[1]
```

## 3.2.3 SVM linear
```{r}
set.seed(3164)
# Train SVM linear
svm.l= train(Cath ~ ., data = svm.train,
            method = "svmLinear",
            trControl = control,
            tuneLength = 10,
            preProcess = c("pca", "scale", "center"),
            metric = "ROC",
            na.action = na.omit)
print(svm.l$results)
```
```{r}
# Test SVM linear
svm.l.predict = predict(svm.l, svm.test)
# confusion matrix
cm.svm.l <- confusionMatrix(svm.l.predict, svm.test$Cath)
print(cm.svm.l)
```
```{r}
result$ROC[3] = max(svm.l$results$ROC)
result$Testing_Accuracy[3] = cm.svm.l$overall[1]
```

```{r}
library("pROC")
```

```{r}
svm.poly.probs = predict(svm.poly,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
svm.rad.probs = predict(svm.rad,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
svm.l.probs = predict(svm.l,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
# plot ROC
svm.poly.ROC = roc(response = svm.test$Cath,
              predictor = svm.poly.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
plot(svm.poly.ROC,type = "S",col = "red")

svm.rad.ROC = roc(response = svm.test$Cath,
              predictor = svm.rad.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
plot(svm.rad.ROC,add = TRUE,col = "green")

svm.l.ROC = roc(response = svm.test$Cath,
              predictor = svm.l.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
plot(svm.l.ROC,add = TRUE,col = "blue")
legend("bottomright", legend = c("svm.poly","svm.rad", "svm.l"), col = c("red", "green", "blue"),lwd = 2)
```
```{r}
result$Test_AUC[1] = svm.poly.ROC$auc
result$Test_AUC[2] = svm.rad.ROC$auc
result$Test_AUC[3] = svm.l.ROC$auc
```

#3.4 Random Forest
```{r}
# train the random forest
rf <- train(Cath ~., data = svm.train,
                 method = "rf",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(rf)
```
```{r}
# Test the random forest
rf.predict = predict(rf, svm.test)
# confusion matrix
cm.rf <- confusionMatrix(rf.predict, svm.test$Cath)
print(cm.rf)
```

```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[4] = max(rf$results$ROC)
result$Testing_Accuracy[4] = cm.rf$overall[1]
# Save the testing ROC for plotting later
rf.probs = predict(rf,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
rf.ROC = roc(response = svm.test$Cath,
              predictor = rf.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
```
3.6 Naive Bayes
```{r}
# train Naive Bayes
nb <- train(Cath ~., data = svm.train,
                 method = "nb",
                 trControl = control,
                 preProc = c("center", "scale"),
                 tuneLength = 10,
                 metric = "ROC")
print(nb)
```

# AdaBoost Classification tree
```{r}
adaB = train(Cath.Cad ~ ., data = im.train,
            method = "AdaBoost.M1",
            tuneGrid = data.frame(mfinal = (1:3)*5, maxdepth = c(5,5,5), coeflearn = c("Breiman", "Freund", "Zhu")),
            preProcess = c("scale", "center"),
            na.action = na.omit)
```

# Testing AdaBoost Claaification tree
```{r}
adab.tree.predict = predict(adaB, im.test)
# confusion matrix
cm.adab.tree <- confusionMatrix(adab.tree.predict, im.test$Cath.Cad)
print(cm.adab.tree)
```

# Boosted Logistic Regression
```{r}
lg.boost = train(Cath.Cad ~ ., data = im.train,
            method = "LogitBoost",
            tuneGrid = data.frame(nIter = c(5,10,20,50)),
            trControl = parameters,
            preProcess = c("scale", "center"),
            na.action = na.omit)
```

# Testing Boosted logistic regression
```{r}
bg.lr.predict = predict(lg.boost, im.test)
# confusion matrix
cm.bg.lr <- confusionMatrix(bg.lr.predict, im.test$Cath.Cad)
print(cm.bg.lr)
```

3.5 Neural Network
```{r}
# train the neural network
cath_index = grep("Cath", colnames(svm.train))
nn = train(svm.train[, -cath_index], 
            svm.train$Cath,
            method = "nnet",
            trControl = control,
            preProcess = c("scale", "center"),
            tuneLength = 5,
            verbose = F,
            trace = F
            metric = "ROC",
            na.action = na.omit)
print(nn)
```
```{r}
# testing the neural network
nn.predict = predict(nn, svm.test)
# confusion matrix
cm.nn <- confusionMatrix(nn.predict, svm.test$Cath)
print(cm.nn)
```

```{r}
# store the ROC and Testing accuracy to the results table
result$ROC[5] = max(nn$results$ROC)
result$Testing_Accuracy[5] = cm.nn$overall[1]
# plot the testing ROC
nn.probs = predict(nn,svm.test[,!names(svm.test) %in% c("Cath")],type = "prob")
nn.ROC = roc(response = svm.test$Cath,
              predictor = nn.probs$Cad,
              levels = levels(svm.test$Cath),
              percent = T)
plot(nn.ROC,add = TRUE,col = "orange")
```