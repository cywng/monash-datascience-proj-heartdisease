---
title: "Bagging & Boosting Models"
author: "Yupeng Miao"
date: "2020/10/8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load pachages & train/test datasets
```{r}
library(ggplot2)
library(caret)
library(xlsx)
```


```{r}
cad <- read.xlsx("DataWrangling/Z-Alizadeh sani dataset.xlsx", 1, header=TRUE)
head(cad)
```
# Factorising variables in char
```{r}
for (i in 1:length(cad)) {
  if (class(cad[,i]) == "character"){
    cad[,i] <- as.factor(cad[,i])
  }
  else if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
```
```{r}
str(cad)
```
We observe that the feature Exertional CP only has one level, so we drop it from the dataset
```{r}
cad <- subset(cad, select = -Exertional.CP)
```

# Catrgorical Random Variables: One-Hot Encoding
```{r}
dummy <- dummyVars(" ~ .", data = cad)
cad <- data.frame(predict(dummy, newdata = cad))
head(cad)
```
# Zero Variance or Near-zero variance variable
```{r}
nz <- nearZeroVar(cad, saveMetrics = TRUE)
nz[nz$nzv,]
```
There are 32 near-zero-variance features which may cause problems when the data are split into cross validation or boostraping samples, we eliminated those features.
```{r}
nzv <- nearZeroVar(cad)
cad <- cad[,-nzv]
str(cad)
```
# Correlated predictors
```{r}
cad_cor <- cor(cad)
high.cor <- sum(abs(cad_cor[upper.tri(cad_cor)])>0.999)
summary(cad_cor[upper.tri(cad_cor)])
```
We observed 17 highly correlated features, many of them are generated by one-hot encoding (e.g. for a variable with 3 levels, drop one of the dummies will not lose any information), we drop them and the effect of removing those with absolute correlations above 0.75 are shown below:
```{r}
high.cor.feature <- findCorrelation(cad_cor, cutoff = 0.75)
high.cor.feature[1] = 65 #remove the Cath.Normal rather than Cath.cad
cad <- cad[, -high.cor.feature]
cad_cor2 <- cor(cad)
summary(cad_cor2[upper.tri(cad_cor2)])
```
# Features with linear dependencies:
```{r}
ld <- findLinearCombos(cad)
ld
```
Nothing observed so we are ok.

# factorising binary variables:
```{r}
for (i in 1:length(cad)) {
  if (cad[,i][1] == 0 | cad[,i][1] == 1 ){
    cad[,i] <- as.factor(cad[,i]) 
  }
}
str(cad)
```

#Train/Test split:

```{r}
set.seed(3164)
train.index <- createDataPartition(cad$Cath.Cad, times = 1, p = 0.8, list = FALSE)
cad.train <- cad[train.index,]
cad.test <- cad[-train.index]
```

# 10-cross-validation
```{r}
parameters = trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 10)
```

# Feature selection by SVM (Not using this)
```{r}
# train SVM
SVM = train(Cath.Cad ~ ., data = cad.train,
            method = "svmPoly",
            trControl = parameters,
            tuneGrid = data.frame(degree = 1, scale = 1, C = 1),
            preProcess = c("pca", "scale", "center"),
            na.action = na.omit)
```
```{r}
# Ranking features by Importance
feature.rank = varImp(SVM, scale = TRUE)
plot(feature.rank)
```
Generate a dataset of features with importance > 40
```{r}
important <- c(feature.rank$importance$X0>40, TRUE)
im <- cad[,important]
str(im)
# train/test split
im.train <- im[train.index,]
im.test <- im[-train.index]
```

# Bagged AdaBoost
```{r}
Bg_adaB = train(Cath.Cad ~ ., data = cad.train,
            method = "AdaBag",
            tuneGrid = data.frame(mfinal = 1000, maxdepth = 5),
            preProcess = c("scale", "center"),
            na.action = na.omit)
```
# Testing Bagged AdaBoost
```{r}
bg.adab.predict = predict(Bg_adaB, cad.test)
# confusion matrix
cm.bg.adab <- confusionMatrix(bg.adab.predict, cad.test$Cath.Cad)
print(cm.bg.adab)
```

# AdaBoost Classification tree
```{r}
adaB = train(Cath.Cad ~ ., data = cad.train,
            method = "AdaBoost.M1",
            tuneGrid = data.frame(mfinal = (1:3)*5, maxdepth = c(5,5,5), coeflearn = c("Breiman", "Freund", "Zhu")),
            preProcess = c("scale", "center"),
            trControl = parameters,
            na.action = na.omit)
```

# Boosted Logistic Regression
```{r}
lg.boost = train(Cath.Cad ~ ., data = cad.train,
            method = "LogitBoost",
            tuneGrid = data.frame(nIter = 10),
            preProcess = c("scale", "center"),
            trControl = parameters,
            na.action = na.omit)
```
```{r}
# test the boosted logistic regression
lg.boost.predict = predict(lg.boost, cad.test)
# confusion matrix
cm.lg.boost <- confusionMatrix(lg.boost.predict, cad.test$Cath.Cad)
print(cm.lg.boost)
```


