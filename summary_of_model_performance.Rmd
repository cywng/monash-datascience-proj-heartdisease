---
title: "Summary of models trained"
author: "Team 1"
date: "09/10/2020"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)
```

# Summary of trained model performance
First we load the models.
```{r}

library(plyr)
library(caret)
library(e1071)
rm(list=ls())
load(file = "DataWrangling/Featuresselected.RData")
load(file = "Models/SVMradmodel.RData")
load(file = "Models/svmpolymodel.RData")
load(file = "Models/svmlinmodel.RData")
load(file = "Models/RF.RData")
load(file = "Models/Neural_Network.RData")
load(file = "Models/NB.RData")
load(file = "Models/LR.RData")
load(file = "Models/LDAmodel.RData")
load(file = "Models/knnmodel.RData")
train.df$Cath <- as.factor(ifelse(train.df$Cath == 0,"N","Y"))
test.df$Cath <- as.factor(ifelse(test.df$Cath == 0,"N","Y"))
```

## Individual Models

Here is a summary of the performance of our individual models.

### k-Nearest Neighbours
```{r}
knn_result <- predict(knn.model, test.df[knn.features])
confusionMatrix(knn_result,test.df$Cath)
```
### Linear Discriminant Analysis
```{r}
lda_result <- predict(lda.model, test.df[lda.features$optVariables])
confusionMatrix(lda_result,test.df$Cath)
```

### Logistic Regression
```{r}
lr_result <- as.factor(ifelse(predict(LR_model, test.df[lr.features$optVariables])==1,"Y","N"))
confusionMatrix(lr_result,test.df$Cath)
```

### Naive Bayes
```{r}
nb_result <- as.factor(ifelse(predict(NB_model, test.df[nb.features$optVariables])==1,"Y","N"))
confusionMatrix(nb_result,test.df$Cath)
```

### Random Forest
```{r}
rf_result <- as.factor(ifelse(predict(RF_model, test.df[rf.features$optVariables])==1,"Y","N"))
confusionMatrix(rf_result,test.df$Cath)
```

### Neural Network
```{r}
nn_result <- as.factor(ifelse(predict(nn1, test.df)==1,"Y","N"))
confusionMatrix(nn_result,test.df$Cath)
```

### Support Vector Machines: Linear kernal
```{r}
svm_lin_result <- predict(svmlin.model, test.df[svmlin.features$optVariables])
confusionMatrix(svm_lin_result,test.df$Cath)
```

### Support Vector Machines: Polynominal kernal
```{r}
svm_poly_result <- predict(svmpoly.model, test.df[svmpoly.features$optVariables])
confusionMatrix(svm_poly_result,test.df$Cath)
```

### Support Vector Machines: Radial Basis kernal
```{r}
svm_rad_result <- predict(svmrad.model, test.df[svmrad.features$optVariables])
confusionMatrix(svm_rad_result,test.df$Cath)
```

## Ensemble Models
We must first define some custom functions. The function vote-ensemble() takes a data set and for every row returns the average of each feature. The function generate_ensemble_df() generates a dataset with features corresponding to the predicted results from the above models, based on their probability. It also aggregates all three SVM features into one weighted predictor, as SVM does not have a probability associated with predictions.
```{r echo=FALSE}
vote_ensemble <- function(dataset, label="Cath", prob='class',input='factor'){
  #label should be string name of column, or FALSE
  #prob = class means we want probabilities out.
  #input = factor means we putting in factors instead of probabilities.
  if(label==FALSE){
    df = dataset
  }  else {
    df = dataset[,names(dataset) != c(label)]
  }
  if(input=='factor'){
    numericdf <- data.frame(apply(df, 2,function(x){revalue(x,c("Y"=1,"N"=0),warn_missing = FALSE)}))
  } else {numericdf <- df}
  num = dim(df)[2]
  vote = apply(numericdf, 1, function(x) sum(as.numeric(x)))/num #change the sum formula here for weignting. Can dot product with weight vector.
  if(prob=='class'){
    return(as.factor(ifelse(round(vote) == 0,"N","Y")))
  } else {return(vote)}
}

generate_ensemble_df <- function(caddataset){
  #models <-  c(knn.model, lda.model, LR_model, NB_model, RF_model, svmlin.model, svmpoly.model, svmrad.model, nn1) this is some fucked up thing
  aggregate_pred.df <- as.data.frame(caddataset$Cath)
  
  #attach predictions
  #only has two??
  aggregate_pred.df$knnres <-  predict(knn.model, caddataset[knn.features], type="prob")$Y
  aggregate_pred.df$ldares <-  predict(lda.model, caddataset[lda.features$optVariables], type="prob")$Y
  aggregate_pred.df$lrres <-  predict(LR_model, caddataset[lr.features$optVariables], type="prob")[,2]
  aggregate_pred.df$nbres <-  predict(NB_model, caddataset[nb.features$optVariables], type="prob")[,2]
  aggregate_pred.df$rfres <-  predict(RF_model, caddataset[rf.features$optVariables], type="prob")[,2]
  colnames(aggregate_pred.df)=c("Cath", "knn", "lda","lr","nb","rf")
  aggregate_pred.df$NNres <-  predict(nn1, caddataset, type="prob")$Y
  
  #SVM has no natural prediction, so we aggregate these 3.
  svm_res_agg <- as.data.frame(predict(svmlin.model, caddataset[svmlin.features$optVariables]))
  svm_res_agg$poly <-  predict(svmpoly.model, caddataset[svmpoly.features$optVariables])
  svm_res_agg$rad <-  predict(svmrad.model, caddataset[svmrad.features$optVariables])
  aggregate_pred.df$svm <- vote_ensemble(svm_res_agg, FALSE, prob='prob',input='factor')
  
  return(aggregate_pred.df)
}
```
```{r}
pred.df <- generate_ensemble_df(train.df)
```

### Ensemble by voting
```{r}
ensem_result_test <- vote_ensemble(generate_ensemble_df(test.df),label = 'Cath', prob='class',input='prob')
confusionMatrix(ensem_result_test,test.df$Cath)
#test accuracy 0.87
```
## Training models on the combined data frame.

### Logistic Regression Ensemble
```{r}
control <- trainControl(method="repeatedcv", number=10)
lr_ensem <- train(Cath ~., data = pred.df, method="glm", family = "binomial" ,trControl=control)

ensem_lr_test=predict(lr_ensem,generate_ensemble_df(test.df))
confusionMatrix(ensem_lr_test,test.df$Cath)
```


```{r}
control <- trainControl(method="repeatedcv", number=10)
rf_ensem<-train(Cath ~., data = pred.df, method="rf", family = "binomial" ,trControl=control)

ensem_rf_test=predict(rf_ensem,generate_ensemble_df(test.df))
confusionMatrix(ensem_rf_test,test.df$Cath)
```